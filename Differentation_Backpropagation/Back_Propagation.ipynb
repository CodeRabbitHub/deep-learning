{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0269219-2940-47a4-8663-243cd32f80fa",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "Backpropagation (backward propagation of errors) is an algorithm used in training artificial neural networks. It involves propagating the error from the output layer back through the network to update the weights, minimizing the error. This process enables the network to learn from training data.\n",
    "\n",
    "### Key Concepts in Backpropagation\n",
    "\n",
    "1. **Forward Pass**: Input data is passed through the network layer by layer to produce an output. During this pass, activations and weighted sums are computed and stored for each layer.\n",
    "\n",
    "2. **Loss Calculation**: The output from the network is compared with the true target values using a loss function (e.g., Mean Squared Error, Cross-Entropy). The loss function quantifies how well the network's output matches the target values.\n",
    "\n",
    "3. **Backward Pass**: The error is propagated back through the network. This involves calculating the gradient of the loss function with respect to each weight in the network, layer by layer, from the output layer back to the input layer.\n",
    "\n",
    "4. **Weight Update**: Using the gradients calculated during the backward pass, the weights are updated to minimize the loss. The update rule typically involves gradient descent, where weights are adjusted in the direction opposite to the gradient of the loss function.\n",
    "\n",
    "### Detailed Steps of Backpropagation\n",
    "\n",
    "1. **Initialization**: Initialize the weights and biases with small random values.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Compute the activations for each layer.\n",
    "   - Store the activations and weighted sums for use in the backward pass.\n",
    "   - Compute Loss: Calculate the loss using the output of the network and the true target values.\n",
    "\n",
    "3. **Backward Pass**:\n",
    "   - Calculate the gradient of the loss with respect to the output of the network.\n",
    "   - Propagate the gradient back through the network:\n",
    "     - Compute the gradient of the loss with respect to the activations and weights for each layer, starting from the output layer and moving backward.\n",
    "     - Use the chain rule to combine gradients at each layer.\n",
    "\n",
    "4. **Update Weights**: Adjust the weights using the gradients computed in the backward pass. The update rule is typically:\n",
    "\n",
    "   $$\n",
    "   w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\eta \\frac{\\partial L}{\\partial w_{ij}^{(l)}}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ w_{ij}^{(l)} $ is the weight from neuron $ i $ to neuron $ j $ in layer $ l $.\n",
    "   - $ \\eta $ is the learning rate.\n",
    "   - $ \\frac{\\partial L}{\\partial w_{ij}^{(l)} } $ is the gradient of the loss with respect to the weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a01e67-9b49-4198-a9b8-443cdcd94c7d",
   "metadata": {},
   "source": [
    "### Example: Backpropagation in a Simple Network\n",
    "\n",
    "Consider a neural network with one input layer, one hidden layer, and one output layer. The hidden layer has two neurons, and the output layer has one neuron.\n",
    "\n",
    "<center><img src=\"fig/network_structure.png\"/></center>\n",
    "\n",
    "#### Network Architecture\n",
    "- **Input Layer**: $ x_1, x_2 $\n",
    "- **Hidden Layer**: $ h_1, h_2 $\n",
    "- **Output Layer**: $ y $\n",
    "\n",
    "#### Weights and biases:\n",
    "- **Input to hidden layer**: $ w_{11}, w_{12}, w_{21}, w_{22} $\n",
    "- **Hidden to output layer**: $ v_1, v_2 $\n",
    "\n",
    "#### Forward Pass, Loss Calculation, Backpropagation, and Weight Update\n",
    "\n",
    "#### Step-by-Step Calculation\n",
    "\n",
    "1. **Forward Pass**\n",
    "\n",
    "   **Compute the outputs of the hidden layer neurons ($h_1$ and $h_2$):**\n",
    "   - $h_1 = w_{11}x_1 + w_{12}x_2$\n",
    "   - $h_2 = w_{21}x_1 + w_{22}x_2$\n",
    "   \n",
    "   **Compute the output $y$:**\n",
    "   - $y = v_1h_1 + v_2h_2$\n",
    "   \n",
    "2. **Loss Calculation**\n",
    "\n",
    "   **Compute the loss $L$:**\n",
    "   - $L = \\frac{1}{2}(y - t)^2$\n",
    "   \n",
    "3. **Backward Pass**\n",
    "\n",
    "   We need to compute the gradients of the loss with respect to each weight.  \n",
    "   **Gradients for $v_1$ and $v_2$ (hidden to output weights):**\n",
    "   Using the chain rule:\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial v_1} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v_1}$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial v_2} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v_2}$\n",
    "   \n",
    "   First, compute $\\frac{\\partial \\mathcal{L}}{\\partial y}$:\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial y} = y - t$\n",
    "   \n",
    "\n",
    "   Next, compute $\\frac{\\partial y}{\\partial v_1}$ and $\\frac{\\partial y}{\\partial v_2}$:\n",
    "   - $\\frac{\\partial y}{\\partial v_1} = h_1$\n",
    "   - $\\frac{\\partial y}{\\partial v_2} = h_2$\n",
    "   \n",
    "   Putting it all together:  \n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial v_1} = (y - t) \\cdot h_1$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial v_2} = (y - t) \\cdot h_2$\n",
    "   \n",
    "   **Gradients for $w_{11}$, $w_{12}$, $w_{21}$, and $w_{22}$ (input to hidden weights):**\n",
    "   Using the chain rule:\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_{11}}$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_{12}}$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{21}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_{21}}$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{22}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_{22}}$\n",
    "   \n",
    "   We already have $\\frac{\\partial \\mathcal{L}}{\\partial y}$:\n",
    "   - $\\frac{\\partial y}{\\partial h_1} = v_1$\n",
    "   - $\\frac{\\partial y}{\\partial h_2} = v_2$\n",
    "   \n",
    "   Next, compute $\\frac{\\partial h_1}{\\partial w_{11}}$, $\\frac{\\partial h_1}{\\partial w_{12}}$, $\\frac{\\partial h_2}{\\partial w_{21}}$, and $\\frac{\\partial h_2}{\\partial w_{22}}$:\n",
    "   - $\\frac{\\partial h_1}{\\partial w_{11}} = x_1$\n",
    "   - $\\frac{\\partial h_1}{\\partial w_{12}} = x_2$\n",
    "   - $\\frac{\\partial h_2}{\\partial w_{21}} = x_1$\n",
    "   - $\\frac{\\partial h_2}{\\partial w_{22}} = x_2$\n",
    "   \n",
    "\n",
    "   Putting it all together:\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} = (y - t) \\cdot v_1 \\cdot x_1$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} = (y - t) \\cdot v_1 \\cdot x_2$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{21}} = (y - t) \\cdot v_2 \\cdot x_1$\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{22}} = (y - t) \\cdot v_2 \\cdot x_2$\n",
    "  \n",
    "4. **Weight Update**\n",
    "   Update the weights using the gradients and a learning rate $\\eta$:\n",
    "   - $v_1^{\\text{new}} = v_1 - \\eta \\frac{\\partial L}{\\partial v_1}$\n",
    "   - $v_2^{\\text{new}} = v_2 - \\eta \\frac{\\partial L}{\\partial v_2}$\n",
    "   - $w_{11}^{\\text{new}} = w_{11} - \\eta \\frac{\\partial L}{\\partial w_{11}}$\n",
    "   - $w_{12}^{\\text{new}} = w_{12} - \\eta \\frac{\\partial L}{\\partial w_{12}}$\n",
    "   - $w_{21}^{\\text{new}} = w_{21} - \\eta \\frac{\\partial L}{\\partial w_{21}}$\n",
    "   - $w_{22}^{\\text{new}} = w_{22} - \\eta \\frac{\\partial L}{\\partial w_{22}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e209-b604-4f04-aa07-5987f1c6d4a3",
   "metadata": {},
   "source": [
    "## Example Calculation\r\n",
    "\r\n",
    "Let's use specific values:\r\n",
    "\r\n",
    "- $x_1 = 1$\r\n",
    "- $x_2 = 2$\r\n",
    "- $t = 5$\r\n",
    "- Initial weights: \r\n",
    "  - $w_{11} = 0.1$\r\n",
    "  - $w_{12} = -0.2$\r\n",
    "  - $w_{21} = 0.3$\r\n",
    "  - $w_{22} = 0.4$\r\n",
    "  - $v_1 = 0.5$\r\n",
    "  - $v_2 = -0.5$\r\n",
    "- Learning rate: $\\eta = 0.01$\r\n",
    "\r\n",
    "### Forward Pass:\r\n",
    "\r\n",
    "- $h_1 = w_{11} \\cdot x_1 + w_{12} \\cdot x_2 = 0.1 \\cdot 1 + (-0.2) \\cdot 2 = 0.1 - 0.4 = -0.3$\r\n",
    "- $h_2 = w_{21} \\cdot x_1 + w_{22} \\cdot x_2 = 0.3 \\cdot 1 + 0.4 \\cdot 2 = 0.3 + 0.8 = 1.1$\r\n",
    "- $y = v_1 \\cdot h_1 + v_2 \\cdot h_2 = 0.5 \\cdot (-0.3) + (-0.5) \\cdot 1.1 = -0.15 - 0.55 = -0.7$\r\n",
    "\r\n",
    "### Loss Calculation:\r\n",
    "\r\n",
    "- $\\mathcal{L} = \\frac{1}{2}(y - t)^2 = \\frac{1}{2}(-0.7 - 5)^2 = \\frac{1}{2} \\cdot 32.49 = 16.245$\r\n",
    "\r\n",
    "### Backward Pass:\r\n",
    "\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial y} = y - t = -0.7 - 5 = -5.7$\r\n",
    "\r\n",
    "#### Gradients for $v_1$ and $v_2$:\r\n",
    "\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial v_1} = (y - t) \\cdot h_1 = -5.7 \\cdot (-0.3) = 1.71$\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial v_2} = (y - t) \\cdot h_2 = -5.7 \\cdot 1.1 = -6.27$\r\n",
    "\r\n",
    "#### Gradients for $w_{11}$, $w_{12}$, $w_{21}$, and $w_{22}$:\r\n",
    "\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} = (y - t) \\cdot v_1 \\cdot x_1 = -5.7 \\cdot 0.5 \\cdot 1 = -2.85$\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} = (y - t) \\cdot v_1 \\cdot x_2 = -5.7 \\cdot 0.5 \\cdot 2 = -5.7$\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{21}} = (y - t) \\cdot v_2 \\cdot x_1 = -5.7 \\cdot (-0.5) \\cdot 1 = 2.85$\r\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{22}} = (y - t) \\cdot v_2 \\cdot x_2 = -5.7 \\cdot (-0.5) \\cdot 2 = 5.7$\r\n",
    "\r\n",
    "### Weight Update:\r\n",
    "\r\n",
    "- $v_1^\\text{new} = v_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial v_1} = 0.5 - 0.01 \\cdot 1.71 = 0.4829$\r\n",
    "- $v_2^\\text{new} = v_2 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial v_2} = -0.5 - 0.01 \\cdot (-6.27) = -0.4373$\r\n",
    "- $w_{11}^\\text{new} = w_{11} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{11}} = 0.1 - 0.01 \\cdot (-2.85) = 0.1285$\r\n",
    "- $w_{12}^\\text{new} = w_{12} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{12}} = -0.2 - 0.01 \\cdot (-5.7) = -0.143$\r\n",
    "- $w_{21}^\\text{new} = w_{21} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{21}} = 0.3 - 0.01 \\cdot 2.85 = 0.2715$\r\n",
    "- $w_{22}^\\text{new} = w_{22} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{22}} = 0.4 - 0.01 \\cdot 5.7 = 0.343$\r\n",
    "\r\n",
    "### After one iteration, the new weights are:\r\n",
    "\r\n",
    "- $v_1 = 0.4829$\r\n",
    "- $v_2 = -0.4373$\r\n",
    "- $w_{11} = 0.1285$\r\n",
    "- $w_{12} = -0.143$\r\n",
    "- $w_{21} = 0.2715$\r\n",
    "- $w_{22} = 0.343$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9bce1-88d3-4a09-a6fc-b6bbc5fe6abb",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "Numerical differentiation approximates the gradient by perturbing the inputs and observing changes in the output. Here, we calculate the gradient numerically for the weights.\n",
    "\n",
    "### Numerical Gradient Calculation\n",
    "\n",
    "We use the central difference method for numerical differentiation. Given a function $ f(w) $, the gradient of $ f $ at $ w $ is approximated by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_i} \\approx \\frac{f(w + \\epsilon e_i) - f(w - \\epsilon e_i)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is a small number (e.g., $ 10^{-5} $) and $ e_i $ is the unit vector in the direction of the $ i $-th parameter.\n",
    "\n",
    "Let's compute the numerical gradient for $ w_{11} $:\n",
    "\n",
    "1. Choose $ \\epsilon = 10^{-5} $.\n",
    "2. Compute $ L(w_{11} + \\epsilon) $ and $ L(w_{11} - \\epsilon) $.\n",
    "3. Let $ x_1 = 1 $, $ x_2 = 2 $, $ t = 5 $, and the initial weights are:\n",
    "   - $ w_{11} = 0.1 $\n",
    "   - $ w_{12} = -0.2 $\n",
    "   - $ w_{21} = 0.3 $\n",
    "   - $ w_{22} = 0.4 $\n",
    "   - $ v_1 = 0.5 $\n",
    "   - $ v_2 = -0.5 $\n",
    "4. **Forward Pass for $ w_{11} + \\epsilon $:**\n",
    "   - $ w_{11} + \\epsilon = 0.10001 $\n",
    "   - $ h_1 = (0.10001) \\cdot 1 + (-0.2) \\cdot 2 = 0.10001 - 0.4 = -0.29999 $\n",
    "   - $ h_2 = 0.3 \\cdot 1 + 0.4 \\cdot 2 = 0.3 + 0.8 = 1.1 $\n",
    "   - $ y = 0.5 \\cdot (-0.29999) + (-0.5) \\cdot 1.1 = -0.149995 - 0.55 = -0.699995 $\n",
    "   - $ L = \\frac{1}{2} \\cdot (-0.699995 - 5)^2 = \\frac{1}{2} \\cdot (25.899975000025) = 12.9499875000125 $\n",
    "5. **Forward Pass for $ w_{11} - \\epsilon $:**\n",
    "   - $ w_{11} - \\epsilon = 0.09999 $\n",
    "   - $ h_1 = (0.09999) \\cdot 1 + (-0.2) \\cdot 2 = 0.09999 - 0.4 = -0.30001 $\n",
    "   - $ h_2 = 0.3 \\cdot 1 + 0.4 \\cdot 2 = 0.3 + 0.8 = 1.1 $\n",
    "   - $ y = 0.5 \\cdot (-0.30001) + (-0.5) \\cdot 1.1 = -0.150005 - 0.55 = -0.700005 $\n",
    "   - $ L = \\frac{1}{2} \\cdot (-0.700005 - 5)^2 = \\frac{1}{2} \\cdot (25.900025000025) = 12.9500125000125 $\n",
    "6. **Numerical Gradient for $ w_{11} $:**\n",
    "   - $ \\frac{\\partial L}{\\partial w_{11}} \\approx \\frac{L(w_{11} + \\epsilon) - L(w_{11} - \\epsilon)}{2\\epsilon} $\n",
    "   - $ \\frac{\\partial L}{\\partial w_{11}} \\approx \\frac{12.9499875000125 - 12.9500125000125}{2 \\times 10^{-5}} $\n",
    "   - $ \\frac{\\partial L}{\\partial w_{11}} \\approx -1.25 $\n",
    "\n",
    "Therefore, the numerical gradient for $ w_{11} $ is approximately $ -1.25 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f40470-e986-4147-8852-9df4a0c20162",
   "metadata": {},
   "source": [
    "## Jacobian Matrix\n",
    "\n",
    "The Jacobian matrix of a vector-valued function $ f $ with respect to a vector $ w $ is a matrix of all first-order partial derivatives of the function. For our neural network, assuming $ f $ represents the network's outputs and $ w $ represents the weights, the Jacobian $ J $ is:\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial w_{11}} & \\frac{\\partial y}{\\partial w_{12}} & \\frac{\\partial y}{\\partial w_{21}} & \\frac{\\partial y}{\\partial w_{22}} & \\frac{\\partial y}{\\partial v_1} & \\frac{\\partial y}{\\partial v_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "From the forward pass, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= v_1 h_1 + v_2 h_2 \\\\\n",
    "h_1 &= w_{11} x_1 + w_{12} x_2 \\\\\n",
    "h_2 &= w_{21} x_1 + w_{22} x_2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y}{\\partial w_{11}} &= v_1 x_1 \\\\\n",
    "\\frac{\\partial y}{\\partial w_{12}} &= v_1 x_2 \\\\\n",
    "\\frac{\\partial y}{\\partial w_{21}} &= v_2 x_1 \\\\\n",
    "\\frac{\\partial y}{\\partial w_{22}} &= v_2 x_2 \\\\\n",
    "\\frac{\\partial y}{\\partial v_1} &= h_1 \\\\\n",
    "\\frac{\\partial y}{\\partial v_2} &= h_2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With the given values:\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "0.5 \\cdot 1 & 0.5 \\cdot 2 & -0.5 \\cdot 1 & -0.5 \\cdot 2 & -0.3 & 1.1\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0.5 & 1 & -0.5 & -1 & -0.3 & 1.1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fa4da-94c8-4009-8751-dacad7252c3f",
   "metadata": {},
   "source": [
    "### Hessian Matrix\n",
    "The Hessian matrix $H$ is a square matrix of second-order partial derivatives of a scalar-valued function. For our loss function $L$, the Hessian $H$ is:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 L}{\\partial w_{11}^2} & \\frac{\\partial^2 L}{\\partial w_{11} \\partial w_{12}} & \\cdots & \\frac{\\partial^2 L}{\\partial w_{11} \\partial v_{2}} \\\\\n",
    "\\frac{\\partial^2 L}{\\partial w_{12} \\partial w_{11}} & \\frac{\\partial^2 L}{\\partial w_{12}^2} & \\cdots & \\frac{\\partial^2 L}{\\partial w_{12} \\partial v_{2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 L}{\\partial v_{2} \\partial w_{11}} & \\frac{\\partial^2 L}{\\partial v_{2} \\partial w_{12}} & \\cdots & \\frac{\\partial^2 L}{\\partial v_{2}^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element of $H$ can be computed using second-order derivatives of $L$. For example, to compute $\\frac{\\partial^2 L}{\\partial w_{11}^2}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w_{11}^2} = \\frac{\\partial}{\\partial w_{11}}\\left(\\frac{\\partial L}{\\partial w_{11}}\\right)\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}} = (y - t) \\cdot v_1 \\cdot x_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w_{11}^2} = \\frac{\\partial}{\\partial w_{11}}\\left((y - t) \\cdot v_1 \\cdot x_1\\right) = v_1 \\cdot x_1 \\cdot \\frac{\\partial (y - t)}{\\partial w_{11}} = v_1 \\cdot x_1 \\cdot v_1 \\cdot x_1 = v_1^2 \\cdot x_1^2\n",
    "$$\n",
    "\n",
    "Similarly, we can compute other elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dcb29-4bc2-4b27-9bea-1769c23bed6f",
   "metadata": {},
   "source": [
    "### Forward Mode Automatic Differentiation\n",
    "\n",
    "Forward mode automatic differentiation propagates derivatives from inputs to outputs. For each input $$x_i$$, we track both the value and the derivative.\n",
    "\n",
    "Let:\n",
    "\n",
    "$$x_1 = 1, \\quad x_2 = 2$$\n",
    "\n",
    "Define initial perturbations:\n",
    "\n",
    "$$\\dot{x}_1 = 1, \\quad \\dot{x}_2 = 0$$\n",
    "\n",
    "Compute $h_1$ and its derivative:\n",
    "\n",
    "$$\n",
    "h_1 = w_{11} \\cdot x_1 + w_{12} \\cdot x_2 = 0.1 \\cdot 1 + (-0.2) \\cdot 2 = -0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{h}_1 = w_{11} \\cdot \\dot{x}_1 + w_{12} \\cdot \\dot{x}_2 = 0.1 \\cdot 1 + (-0.2) \\cdot 0 = 0.1\n",
    "$$\n",
    "\n",
    "Compute $h_2$ and its derivative:\n",
    "\n",
    "$$\n",
    "h_2 = w_{21} \\cdot x_1 + w_{22} \\cdot x_2 = 0.3 \\cdot 1 + 0.4 \\cdot 2 = 1.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{h}_2 = w_{21} \\cdot \\dot{x}_1 + w_{22} \\cdot \\dot{x}_2 = 0.3 \\cdot 1 + 0.4 \\cdot 0 = 0.3\n",
    "$$\n",
    "\n",
    "Compute $y$ and its derivative:\n",
    "\n",
    "$$\n",
    "y = v_1 \\cdot h_1 + v_2 \\cdot h_2 = 0.5 \\cdot (-0.3) + (-0.5) \\cdot 1.1 = -0.7\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{y} = v_1 \\cdot \\dot{h}_1 + v_2 \\cdot \\dot{h}_2 = 0.5 \\cdot 0.1 + (-0.5) \\cdot 0.3 = 0.05 - 0.15 = -0.1\n",
    "$$\n",
    "\n",
    "### Reverse Mode Automatic Differentiation\n",
    "\n",
    "Reverse mode automatic differentiation propagates derivatives from outputs to inputs, effectively computing gradients in a backward pass.\n",
    "\n",
    "Start with $y$ and backpropagate gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = y - t = -0.7 - 5 = -5.7\n",
    "$$\n",
    "\n",
    "Backpropagate to hidden layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_1} = -5.7 \\cdot v_1 = -5.7 \\cdot 0.5 = -2.85\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_2} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} = -5.7 \\cdot v_2 = -5.7 \\cdot (-0.5) = 2.85\n",
    "$$\n",
    "\n",
    "Backpropagate to input layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_{11}} = -2.85 \\cdot x_1 = -2.85 \\cdot 1 = -2.85\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{12}} = \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_{12}} = -2.85 \\cdot x_2 = -2.85 \\cdot 2 = -5.7\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_{21}} = 2.85 \\cdot x_1 = 2.85 \\cdot 1 = 2.85\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{22}} = \\frac{\\partial L}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_{22}} = 2.85 \\cdot x_2 = 2.85 \\cdot 2 = 5.7\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v_1} = -5.7 \\cdot h_1 = -5.7 \\cdot (-0.3) = 1.71\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_2} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v_2} = -5.7 \\cdot h_2 = -5.7 \\cdot 1.1 = -6.27\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c342a-d9d3-4edd-8fed-5e9d862bcf67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
