{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71189102-4e6e-42b2-9e6b-573d8fc14323",
   "metadata": {},
   "source": [
    "### Weight Decay\n",
    "\n",
    "**Weight decay** is a regularization technique used to prevent overfitting in machine learning models, particularly neural networks. It works by adding a penalty to the loss function, which discourages large weights in the model. The most common form of weight decay is L2 regularization.\n",
    "\n",
    "#### L2 Regularization (Weight Decay)\n",
    "\n",
    "The objective function with L2 regularization is:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|_2^2\n",
    "$$\n",
    "where:\n",
    "- $\\mathcal{L}_{\\text{original}}(\\mathbf{w})$ is the original loss function (e.g., mean squared error or cross-entropy loss).\n",
    "- $\\mathbf{w}$ represents the weights of the model.\n",
    "- $\\lambda$ is the regularization parameter that controls the strength of the penalty.\n",
    "- $\\|\\mathbf{w}\\|_2^2 = \\sum_{i} w_i^2$ is the L2 norm of the weight vector.\n",
    "\n",
    "The gradient of this regularized loss with respect to the weights is:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda \\mathbf{w}\n",
    "$$\n",
    "This shows that during gradient descent, the weight update rule becomes:\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta (\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda \\mathbf{w})\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "### Consistent Regularizers\n",
    "\n",
    "**Consistent regularizers** are designed to ensure that the regularization term does not interfere with the model's ability to learn the underlying data distribution. This consistency often means that the regularization term should not dominate the primary loss term, ensuring that the model remains focused on minimizing the actual error rather than the regularization penalty.\n",
    "\n",
    "For instance, in the case of dropout, dropout regularization applies a regularization effect during training by randomly dropping out (i.e., setting to zero) a proportion of the neurons. This means that the contribution of these neurons to the forward pass and backpropagation is temporarily removed. However, during inference (i.e., when making predictions), all neurons are used. To compensate for the dropped-out neurons during training, the weights are scaled during inference to reflect the expected sum of activations. Dropout doesn't add a term to the loss function directly but modifies the training process to achieve a regularization effect.\n",
    "\n",
    "### Generalized Weight Decay\n",
    "\n",
    "**Generalized weight decay** extends the idea of traditional weight decay by applying different types of norms or penalties to the weights. This can involve norms other than the L2 norm or even non-norm-based penalties.\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "\n",
    "L1 regularization penalizes the absolute values of the weights, leading to sparse models (i.e., many weights become zero):\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_1\n",
    "$$\n",
    "where $\\|\\mathbf{w}\\|_1 = \\sum_{i} |w_i|$.\n",
    "\n",
    "The gradient with respect to the weights is:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda \\text{sign}(\\mathbf{w})\n",
    "$$\n",
    "where $\\text{sign}(\\mathbf{w})$ is the element-wise sign function.\n",
    "\n",
    "#### Elastic Net Regularization\n",
    "\n",
    "Elastic Net combines L1 and L2 regularizations:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda_1 \\|\\mathbf{w}\\|_1 + \\frac{\\lambda_2}{2} \\|\\mathbf{w}\\|_2^2\n",
    "$$\n",
    "\n",
    "#### Other Penalty Forms\n",
    "\n",
    "Other forms of generalized weight decay can involve penalties like:\n",
    "- Group Lasso: Penalties applied to groups of weights to encourage group sparsity.\n",
    "- Total Variation: Penalties based on differences between neighboring weights.\n",
    "\n",
    "#### Mathematical Generalization\n",
    "\n",
    "In a generalized form, weight decay can be seen as adding a regularization term $\\Omega(\\mathbf{w})$ to the loss function:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\mathcal{L}_{\\text{original}}(\\mathbf{w}) + \\lambda \\Omega(\\mathbf{w})\n",
    "$$\n",
    "where $\\Omega(\\mathbf{w})$ could be any function that imposes the desired regularization properties on the weights.\n",
    "\n",
    "Each of these concepts helps in improving the generalization ability of machine learning models by controlling the complexity of the model and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b922b-f7d4-4d01-b5b8-dc79d4348b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
