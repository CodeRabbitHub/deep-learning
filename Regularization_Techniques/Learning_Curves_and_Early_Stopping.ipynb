{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422cd4ea-ad28-4de0-9031-eae0c39fc932",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "**Learning curves** are graphical representations that show the performance of a machine learning model over time as it learns from more data. They are used to diagnose whether a model is underfitting or overfitting. The two main types of learning curves are:\n",
    "\n",
    "1. **Training Learning Curve**: Shows the error (or accuracy) of the model on the training dataset as a function of the number of training iterations or the size of the training set.\n",
    "2. **Validation Learning Curve**: Shows the error (or accuracy) of the model on a validation dataset as a function of the number of training iterations or the size of the training set.\n",
    "\n",
    "The general shape of these curves can give insights into the model's performance:\n",
    "- If both the training and validation errors are high and decreasing, the model is underfitting.\n",
    "- If the training error is low but the validation error is high and increasing, the model is overfitting.\n",
    "\n",
    "Mathematically, if $L_{\\text{train}}(m)$ is the training loss and $L_{\\text{val}}(m)$ is the validation loss as a function of the training set size $m$, then these functions describe the learning curves.\n",
    "\n",
    "<center><img src=\"fig/learning_curves.png\"/></center>\n",
    "\n",
    "1. Training error as a function of training set size $m$:\n",
    "   $$\n",
    "   L_{\\text{train}}(m) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(y_i, \\hat{y}_i)\n",
    "   $$\n",
    "   where $\\mathcal{L}$ is the loss function, $y_i$ is the true label, and $\\hat{y}_i$ is the predicted label.\n",
    "\n",
    "2. Validation error as a function of training set size $m$:\n",
    "   $$\n",
    "   L_{\\text{val}}(m) = \\frac{1}{|\\mathcal{D}_{\\text{val}}|} \\sum_{i=1}^{|\\mathcal{D}_{\\text{val}}|} \\mathcal{L}(y_i, \\hat{y}_i)\n",
    "   $$\n",
    "   where $\\mathcal{D}_{\\text{val}}$ is the validation dataset.\n",
    "\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "**Early stopping** is a technique used to prevent overfitting in iterative training processes, such as gradient descent for neural networks. The idea is to stop training when the performance on a validation set starts to deteriorate, even if the training error is still decreasing. This helps in keeping the model generalized and avoids overfitting.\n",
    "\n",
    "<center><img src=\"fig/early_stopping.png\"/></center>\n",
    "\n",
    "Mathematically, consider the validation error $L_{\\text{val}}(t)$ at iteration $t$. Early stopping involves monitoring $L_{\\text{val}}(t)$ and halting the training when:\n",
    "$$\n",
    "L_{\\text{val}}(t) > L_{\\text{val}}(t - \\Delta t)\n",
    "$$\n",
    "for some patience parameter $\\Delta t$. The patience parameter determines how many iterations to wait before stopping after the validation error has stopped improving.\n",
    "\n",
    "### Double Descent\n",
    "\n",
    "**Double descent** is a phenomenon observed in modern machine learning, particularly with deep learning models, where the test error as a function of model complexity or training epochs shows a \"double descent\" behavior. Initially, as model complexity increases, the test error decreases, reaches a minimum, and then starts increasing (classical bias-variance tradeoff). Surprisingly, if the model complexity is increased further, the test error starts decreasing again after a certain point.\n",
    "\n",
    "This can be visualized in two forms:\n",
    "\n",
    "\n",
    "1. **Model complexity double descent**: Plotting test error against model complexity (e.g., the number of parameters in a neural network).\n",
    "   $$\n",
    "   E_{\\text{test}}(h) = f(\\text{model complexity})\n",
    "   $$\n",
    "   where $h$ represents model complexity, such as the number of parameters.\n",
    "\n",
    "2. **Epoch-wise double descent**: Plotting test error against the number of training epochs.\n",
    "   $$\n",
    "   E_{\\text{test}}(t) = f(\\text{training epochs})\n",
    "   $$\n",
    "   where $t$ represents the number of training epochs.\n",
    "\n",
    "\n",
    "<center><img src=\"fig/double-descent-curve.png\"/></center>\n",
    "\n",
    "In summary, learning curves help visualize the learning process and diagnose overfitting or underfitting, early stopping is a practical technique to prevent overfitting by halting training at the right moment, and double descent is a modern phenomenon highlighting the non-monotonic behavior of test error with increasing model complexity or training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bddbb23-885c-44db-a52a-e4f4a4ba332b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
