{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54063563-ce3c-476c-a5f8-5215ce1e5f34",
   "metadata": {},
   "source": [
    "### Parameter Sharing\n",
    "\n",
    "Parameter sharing is a technique used primarily in convolutional neural networks (CNNs) to reduce the number of parameters and thus the computational complexity of the model. It ensures that the same weights (parameters) are used for different parts of the input, leading to a more efficient and generalizable model.\n",
    "\n",
    "#### Explanation with Convolutional Neural Networks (CNNs)\n",
    "\n",
    "In a CNN, a filter (or kernel) convolves over the input image to produce a feature map. The filter consists of a small matrix of weights, and this same filter is applied across different spatial locations of the input image. This is the essence of parameter sharing: the same set of weights is used for every region of the image.\n",
    "\n",
    "Mathematically, consider an input image $ I $ of size $ H \\times W $ and a filter $ F $ of size $ k \\times k $. The output feature map $ O $ at position $(i, j)$ is computed as:\n",
    "\n",
    "$$ O_{i,j} = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I_{i+m, j+n} \\cdot F_{m,n} $$\n",
    "\n",
    "where $ F $ is the same for all positions $(i, j)$.\n",
    "\n",
    "In fully connected layers, each weight is unique to its connection. If the input has $ H \\times W $ neurons and there are $ N $ neurons in the hidden layer, the total number of parameters is $ H \\times W \\times N $.\n",
    "\n",
    "In contrast, with parameter sharing in a convolutional layer, if we use $ M $ filters of size $ k \\times k $, the total number of parameters is $ M \\times k \\times k $, which is significantly smaller.\n",
    "\n",
    "#### Benefits of Parameter Sharing\n",
    "\n",
    "1. **Reduced Number of Parameters**: Reduces the memory and computational requirements.\n",
    "2. **Translation Invariance**: Enables the network to detect features regardless of their spatial location.\n",
    "3. **Better Generalization**: Helps in preventing overfitting due to fewer parameters.\n",
    "\n",
    "### Soft Weight Sharing\n",
    "\n",
    "Soft weight sharing is a regularization technique used to encourage weights in a neural network to take on similar values. This helps in reducing overfitting and improving generalization.\n",
    "\n",
    "#### Explanation with Mathematical Formulation\n",
    "\n",
    "Soft weight sharing can be thought of as clustering the weights of the neural network into a few groups, where weights within a group are similar but not identical.\n",
    "\n",
    "1. **Probabilistic Interpretation**: We assume that the weights are drawn from a mixture of Gaussian distributions. Each weight $ w_i $ is drawn from a distribution:\n",
    "\n",
    "   $$ w_i \\sim \\sum_{j} \\pi_j \\mathcal{N}(\\mu_j, \\sigma_j^2) $$\n",
    "\n",
    "   where $ \\pi_j $ is the mixing coefficient, and $ \\mathcal{N}(\\mu_j, \\sigma_j^2) $ is a Gaussian distribution with mean $ \\mu_j $ and variance $ \\sigma_j^2 $.\n",
    "\n",
    "2. **Regularization Term**: The log-likelihood of this model can be used as a regularization term. If $ \\theta $ represents the parameters of the neural network and $ \\mathbf{w} $ are the weights, the regularization term is:\n",
    "\n",
    "   $$ \\mathcal{L}_{\\text{reg}}(\\mathbf{w}) = \\sum_{i} \\log \\left( \\sum_{j} \\pi_j \\mathcal{N}(w_i; \\mu_j, \\sigma_j^2) \\right) $$\n",
    "\n",
    "3. **Objective Function**: The total objective function combines the standard loss function (e.g., cross-entropy) with this regularization term:\n",
    "\n",
    "   $$ \\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{loss}}(\\theta) + \\lambda \\mathcal{L}_{\\text{reg}}(\\mathbf{w}) $$\n",
    "\n",
    "   where $ \\lambda $ is a hyperparameter controlling the strength of the regularization.\n",
    "\n",
    "#### Benefits of Soft Weight Sharing\n",
    "\n",
    "1. **Regularization**: Acts as a form of regularization, reducing overfitting.\n",
    "2. **Parameter Efficiency**: Encourages weight sharing, leading to models that can potentially have fewer unique parameters.\n",
    "3. **Model Interpretability**: Clustering weights can sometimes lead to more interpretable models.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Parameter Sharing**: Involves using the same parameters (weights) for different parts of the input. It is most commonly used in CNNs to reduce the number of parameters and improve generalization.\n",
    "- **Soft Weight Sharing**: Encourages weights in the network to be similar by modeling them as being drawn from a mixture of Gaussians. This acts as a regularizer to improve the modelâ€™s generalization capability.\n",
    "\n",
    "Both techniques aim to reduce the complexity of neural networks, making them more efficient and less prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6660875-bea0-4e5f-a6e1-4d1929ee590d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
