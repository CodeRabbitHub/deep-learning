{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024a8615-1c8f-45c4-b3ff-7dd0c6810e95",
   "metadata": {},
   "source": [
    "# Residual Connections\n",
    "\n",
    "Residual connections, also known as skip connections, are a key component of Residual Networks (ResNets), a type of neural network architecture designed to address the degradation problem in deep networks. The degradation problem refers to the phenomenon where adding more layers to a deep network leads to worse performance, even when the network is properly trained. Residual connections help mitigate this issue by allowing the network to learn residual functions with reference to the layer inputs, rather than learning unreferenced functions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "1. **Basic Concept**:\n",
    "   \n",
    "   In a typical neural network layer, you have an input $ \\mathbf{x} $ and an output $ \\mathbf{y} $ that is transformed by a function $ \\mathcal{F} $:\n",
    "   \n",
    "   $$ \\mathbf{y} = \\mathcal{F}(\\mathbf{x}) $$\n",
    "\n",
    "   In ResNets, instead of learning the direct mapping $ \\mathcal{F}(\\mathbf{x}) $, the network learns the residual mapping $ \\mathcal{F}(\\mathbf{x}) - \\mathbf{x} $. This leads to the following formulation:\n",
    "   \n",
    "   $$ \\mathbf{y} = \\mathcal{F}(\\mathbf{x}) + \\mathbf{x} $$\n",
    "\n",
    "   Here, $ \\mathbf{x} $ is the input to the residual block, $ \\mathcal{F}(\\mathbf{x}) $ is the learned residual function, and the addition of $ \\mathbf{x} $ is the residual connection.\n",
    "\n",
    "2. **Layer Formulation**:\n",
    "\n",
    "   For a given input $ \\mathbf{x} $ to a residual block, the output $ \\mathbf{y} $ is given by:\n",
    "   \n",
    "   $$ \\mathbf{y} = \\sigma(\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}) $$\n",
    "\n",
    "   where $ \\sigma $ is an activation function (e.g., ReLU), and $ \\mathcal{F}(\\mathbf{x}) $ is typically composed of one or more convolutional layers, batch normalization, and activation functions.\n",
    "\n",
    "3. **Deep Residual Learning**:\n",
    "   \n",
    "   For a deep residual network with $ L $ layers, let the input to the $ l $-th layer be $ \\mathbf{x}_l $. The residual connection for layer $ l $ can be represented as:\n",
    "   \n",
    "   $$ \\mathbf{x}_{l+1} = \\mathbf{x}_l + \\mathcal{F}(\\mathbf{x}_l, \\{W_l\\}) $$\n",
    "\n",
    "   Here, $ \\{W_l\\} $ denotes the weights of the $ l $-th layer. If there are multiple layers within a residual block, $ \\mathcal{F} $ can represent a composition of multiple transformations.\n",
    "\n",
    "4. **Benefits**:\n",
    "\n",
    "   - **Gradient Flow**: Residual connections improve the flow of gradients through the network, making it easier to train very deep networks. During backpropagation, gradients can flow directly through the identity connections, mitigating the vanishing gradient problem.\n",
    "   \n",
    "   - **Expressiveness**: The residual connections allow the network to learn perturbations from the identity mapping, which is often easier than learning the complete mapping from scratch.\n",
    "\n",
    "### Detailed Example\n",
    "\n",
    "Consider a simple residual block with two layers. Let the input to the block be $ \\mathbf{x} $, and the block consists of two convolutional layers with ReLU activations. The block can be represented as follows:\n",
    "\n",
    "1. **First Layer**:\n",
    "   \n",
    "   $$ \\mathbf{h}_1 = \\sigma(W_1 \\mathbf{x} + b_1) $$\n",
    "\n",
    "2. **Second Layer**:\n",
    "   \n",
    "   $$ \\mathbf{h}_2 = W_2 \\mathbf{h}_1 + b_2 $$\n",
    "\n",
    "3. **Residual Connection**:\n",
    "   \n",
    "   $$ \\mathbf{y} = \\mathbf{h}_2 + \\mathbf{x} $$\n",
    "\n",
    "   where $ W_1 $ and $ W_2 $ are the weights of the convolutional layers, $ b_1 $ and $ b_2 $ are the biases, and $ \\sigma $ is the ReLU activation function.\n",
    "\n",
    "Putting it together:\n",
    "\n",
    "$$ \\mathbf{y} = W_2 (\\sigma(W_1 \\mathbf{x} + b_1)) + b_2 + \\mathbf{x} $$\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Identity Mapping**: If the learned residual function $ \\mathcal{F}(\\mathbf{x}) $ is zero, the output $ \\mathbf{y} $ will be equal to the input $ \\mathbf{x} $. This implies that the network can easily retain the input through the identity mapping if necessary.\n",
    "  \n",
    "- **Residual Learning**: The network is encouraged to learn small adjustments (residuals) to the identity mapping, which can be more efficient and effective, especially in very deep networks.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "In a typical residual block, the input $ \\mathbf{x} $ is passed through a series of transformations to produce $ \\mathcal{F}(\\mathbf{x}) $. The original input $ \\mathbf{x} $ is then added to this output:\n",
    "\n",
    "<center><img src=\"fig/residual.png\"/></center>\n",
    "\n",
    "\n",
    "This skip connection directly adds the input $ \\mathbf{x} $ to the output of the transformations, facilitating the learning of residual functions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Residual connections have proven to be a powerful architectural feature for deep neural networks, enabling the training of significantly deeper models by addressing the vanishing gradient problem and improving gradient flow. The mathematical foundation of residual connections lies in learning the residual functions relative to the identity mapping, making it easier to optimize deep networks and achieve better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56652a7b-85f0-41a2-9310-d21592df7d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
