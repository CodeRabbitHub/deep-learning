{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c4e5f1-c2db-4ff1-b5e8-5ebc83a837a2",
   "metadata": {},
   "source": [
    "### 1. Inductive Bias\n",
    "\n",
    "**Inductive bias** refers to the set of assumptions a learning algorithm makes to predict outputs given inputs that it has not encountered before. These biases are crucial because they guide the learning process and help generalize from the training data to unseen instances.\n",
    "\n",
    "#### Example and Explanation:\n",
    "\n",
    "Consider a machine learning algorithm that aims to learn a function $ f $ from a dataset $ D $ consisting of input-output pairs $ (x, y) $.\n",
    "\n",
    "$$ D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\} $$\n",
    "\n",
    "To predict $ y $ for a new input $ x $, the algorithm relies on an inductive bias, which can be thought of as constraints or assumptions about the form of the function $ f $. Common types of inductive bias include:\n",
    "\n",
    "- **Linear bias:** The assumption that the relationship between $ x $ and $ y $ is linear.\n",
    "- **Smoothness bias:** The assumption that small changes in $ x $ result in small changes in $ y $.\n",
    "- **Simplicity bias:** The preference for simpler models (e.g., fewer parameters).\n",
    "\n",
    "#### Mathematical Example:\n",
    "\n",
    "Suppose we assume a linear relationship between $ x $ and $ y $:\n",
    "\n",
    "$$ y = f(x) = w_0 + w_1 x $$\n",
    "\n",
    "Here, the inductive bias is the assumption that $ f $ is a linear function. The learning algorithm will then try to find the weights $ w_0 $ and $ w_1 $ that best fit the training data.\n",
    "\n",
    "### 2. Inverse Problems\n",
    "\n",
    "An **inverse problem** involves determining the cause from the observed effects. In mathematical terms, if we have a forward model that maps cause $ x $ to effect $ y $ via a function $ f $:\n",
    "\n",
    "$$ y = f(x) $$\n",
    "\n",
    "The inverse problem seeks to determine $ x $ given $ y $. These problems are often ill-posed, meaning they do not have a unique solution or the solution may not depend continuously on the data.\n",
    "\n",
    "#### Example and Explanation:\n",
    "\n",
    "Consider a simple linear model:\n",
    "\n",
    "$$ y = Ax $$\n",
    "\n",
    "where $ A $ is a known matrix, and $ x $ is the unknown vector we want to determine from the observation $ y $. The inverse problem is to solve for $ x $ given $ y $ and $ A $.\n",
    "\n",
    "#### Mathematical Solution:\n",
    "\n",
    "To solve for $ x $, we use the pseudoinverse of $ A $:\n",
    "\n",
    "$$ x = A^+ y $$\n",
    "\n",
    "where $ A^+ $ is the Moore-Penrose pseudoinverse of $ A $. This provides a least-squares solution to the inverse problem, minimizing the error in a least-squares sense.\n",
    "\n",
    "### 3. No Free Lunch Theorem\n",
    "\n",
    "The **No Free Lunch Theorem** (NFL) states that no learning algorithm performs better than any other when averaged over all possible problems. This implies that there is no universally superior algorithm for all types of problems.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "\n",
    "Let $ \\mathcal{A} $ be an algorithm, $ \\mathcal{D} $ a dataset, and $ L $ a loss function. The NFL theorem can be expressed as:\n",
    "\n",
    "$$ \\sum_{P} \\mathbb{E}_{\\mathcal{D} \\sim P}[L(\\mathcal{A}(\\mathcal{D}))] = \\text{constant} $$\n",
    "\n",
    "where $ P $ ranges over all possible data distributions.\n",
    "\n",
    "This implies that for any two algorithms $ \\mathcal{A}_1 $ and $ \\mathcal{A}_2 $:\n",
    "\n",
    "$$ \\sum_{P} \\mathbb{E}_{\\mathcal{D} \\sim P}[L(\\mathcal{A}_1(\\mathcal{D}))] = \\sum_{P} \\mathbb{E}_{\\mathcal{D} \\sim P}[L(\\mathcal{A}_2(\\mathcal{D}))] $$\n",
    "\n",
    "### 4. Symmetry and Invariance\n",
    "\n",
    "**Symmetry** in mathematics and physics refers to an object being invariant under certain transformations, such as rotation, reflection, or translation.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "A function $ f(x) $ is symmetric with respect to the origin if:\n",
    "\n",
    "$$ f(-x) = f(x) $$\n",
    "\n",
    "**Invariance** is a property where certain transformations do not affect the outcome of a function or system.\n",
    "\n",
    "#### Example and Explanation:\n",
    "\n",
    "In machine learning, an invariant function is one whose output does not change under certain transformations of the input. For example, an image classification model might be invariant to translations of the image.\n",
    "\n",
    "### 5. Equivariance\n",
    "\n",
    "**Equivariance** is a property where applying a transformation to the input results in a corresponding transformation to the output. Formally, a function $ f $ is equivariant with respect to a group of transformations $ \\mathcal{G} $ if:\n",
    "\n",
    "$$ f(T_g(x)) = T'_g(f(x)) $$\n",
    "\n",
    "for all transformations $ T_g \\in \\mathcal{G} $ and corresponding transformations $ T'_g $.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a convolutional neural network (CNN) used for image processing. Convolutional layers are translation-equivariant because a translation of the input image results in a corresponding translation of the feature maps.\n",
    "\n",
    "#### Mathematical Explanation:\n",
    "\n",
    "Let $ x $ be an input image and $ f $ a convolution operation. If $ T $ represents a translation operator, then:\n",
    "\n",
    "$$ f(T(x)) = T(f(x)) $$\n",
    "\n",
    "This means that the feature map produced by the convolution operation will be shifted in the same way as the input image.\n",
    "\n",
    "---\n",
    "\n",
    "By understanding these fundamental concepts, we can better grasp how machine learning algorithms function, how they generalize from data, and the limitations and properties they inherit from their underlying assumptions and mathematical frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a535f-5355-479e-95fc-5b82d544c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
