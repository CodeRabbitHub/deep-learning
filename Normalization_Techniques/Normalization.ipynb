{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8436c4-1cc9-4fad-8c1c-4e580a83eb11",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "**Definition:**\n",
    "Data normalization is a preprocessing step applied to numerical data to adjust the values to a common scale without distorting differences in the ranges of values. The goal is to make different features contribute equally to the modelâ€™s learning process.\n",
    "\n",
    "**Types of Data Normalization:**\n",
    "\n",
    "1. **Min-Max Scaling:**\n",
    "   - Rescales the data to a fixed range, typically $[0, 1]$ or $[-1, 1]$.\n",
    "   - Formula: $X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}$\n",
    "   - **When to Use:** Useful when you know the minimum and maximum values of your data. Works well when the distribution is not Gaussian or if there are no outliers.\n",
    "\n",
    "2. **Z-Score Normalization (Standardization):**\n",
    "   - Transforms the data to have a mean of 0 and a standard deviation of 1.\n",
    "   - Formula: $X' = \\frac{X - \\mu}{\\sigma}$\n",
    "   - **When to Use:** Preferred when data follows a Gaussian distribution. Helps in achieving convergence faster for many machine learning algorithms, particularly those that assume a Gaussian distribution of the input data (e.g., linear regression, logistic regression).\n",
    "\n",
    "**Why Use Data Normalization:**\n",
    "\n",
    "- **Improves Convergence:** Gradient descent converges faster with normalized data because the scale of the features is similar.\n",
    "- **Prevents Dominance:** Prevents features with larger scales from dominating the learning process.\n",
    "- **Improves Accuracy:** Can improve the accuracy and performance of the model.\n",
    "\n",
    "**How to Use:**\n",
    "\n",
    "Normalize features before feeding them into the model using libraries like `scikit-learn`:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()  # or StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db4aab-3332-417c-a373-28c15f8ed208",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "**Definition:**\n",
    "Batch normalization (BN) is a technique to improve the training of deep neural networks by normalizing the input of each layer within a mini-batch, thus ensuring a stable distribution of activations throughout training.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "- Normalizes the output of a layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "- Applies learnable scaling ($\\gamma$) and shifting ($\\beta$) parameters to allow the model to undo normalization if needed.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "  $$\n",
    "  $$\n",
    "  y = \\gamma \\hat{x} + \\beta\n",
    "  $$\n",
    "  where $\\mu_B$ and $\\sigma_B$ are the mean and variance of the batch, $\\epsilon$ is a small constant to avoid division by zero, and $\\gamma$, $\\beta$ are learnable parameters.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "- During the training of deep neural networks to mitigate the problem of internal covariate shift.\n",
    "- Commonly used in convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "\n",
    "**Why Use:**\n",
    "\n",
    "- **Stabilizes Learning:** Helps in stabilizing and accelerating the training process.\n",
    "- **Allows Higher Learning Rates:** Reduces the sensitivity to the initial weights.\n",
    "- **Regularizes the Model:** Provides slight regularization, reducing the need for dropout.\n",
    "\n",
    "**How to Use:**\n",
    "\n",
    "Typically used after the activation function in a layer, implemented in deep learning frameworks:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example of using BatchNormalization in PyTorch\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 50)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize and use the network\n",
    "model = NeuralNetwork()\n",
    "input_data = torch.randn(5, 10)\n",
    "output = model(input_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe6be8-691e-4a38-9f0c-ecdc1ce2c09c",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "**Definition:**\n",
    "Layer normalization (LN) normalizes the inputs across the features for each individual data point rather than across the mini-batch. It is especially useful in recurrent neural networks and models where batch sizes are very small.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "- Normalizes the inputs across the features dimension.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $$\n",
    "  $$\n",
    "  y = \\gamma \\hat{x} + \\beta\n",
    "  $$\n",
    "  where $\\mu$ and $\\sigma$ are the mean and variance calculated across the features, $\\epsilon$ is a small constant to avoid division by zero, and $\\gamma$, $\\beta$ are learnable parameters.\n",
    "\n",
    "**When to Use:**\n",
    "\n",
    "- Particularly useful in models with small batch sizes or RNNs where the dependence on sequential data can disrupt the utility of batch normalization.\n",
    "- Used in Transformer models and other architectures where the variance within the batch might not be representative.\n",
    "\n",
    "**Why Use:**\n",
    "\n",
    "- **Consistent Performance:** Provides more stable performance across different batch sizes.\n",
    "- **Improves Gradient Flow:** Helps in maintaining healthy gradient flows, especially in deeper networks and RNNs.\n",
    "- **Independent of Batch Size:** Unlike batch normalization, its performance is independent of the batch size.\n",
    "\n",
    "**How to Use:**\n",
    "\n",
    "Implemented in deep learning frameworks:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example of using LayerNormalization in PyTorch\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 50)\n",
    "        self.layer_norm1 = nn.LayerNorm(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize and use the network\n",
    "model = NeuralNetwork()\n",
    "input_data = torch.randn(5, 10)\n",
    "output = model(input_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1f15e-d2ac-435a-83d9-7783a7300f70",
   "metadata": {},
   "source": [
    "### Comparison and Use Cases\n",
    "\n",
    "- **Data Normalization:** Preprocessing step for normalizing input data, beneficial for traditional machine learning models and neural networks.\n",
    "- **Batch Normalization:** Applied during training of deep networks to stabilize learning, particularly in CNNs and large-batch scenarios.\n",
    "- **Layer Normalization:** Preferred in settings with small batch sizes, RNNs, or Transformer models to ensure stable activations and gradient flows.\n",
    "\n",
    "**Problem Solved:**\n",
    "\n",
    "- **Internal Covariate Shift:** Batch normalization addresses this by stabilizing the distribution of inputs to each layer.\n",
    "- **Gradient Flow:** Layer normalization helps maintain effective gradient flow in deep networks and sequences.\n",
    "\n",
    "By understanding when and why to use each normalization technique, you can significantly enhance the performance and stability of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80828887-2ebb-4a3b-99cb-0303eec20b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
